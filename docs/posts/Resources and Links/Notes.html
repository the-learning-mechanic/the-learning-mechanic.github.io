<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Manuel Pardo">
<meta name="dcterms.date" content="2023-09-20">
<meta name="description" content="A Highlevel look at Optimizers and Schedulers">

<title>Manuel Pardo - Study Notes-Optimizers and Schedulers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Manuel Pardo</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/the-learning-mechanic" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/mannypardo" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/C0untr4G3nt" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Study Notes-Optimizers and Schedulers</h1>
                  <div>
        <div class="description">
          A Highlevel look at Optimizers and Schedulers
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Study Notes</div>
                <div class="quarto-category">Optimizers</div>
                <div class="quarto-category">Schedulers</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Manuel Pardo </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 20, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>In machine learning and deep learning, an optimizer’s primary role is to update model parameters during training in order to minimize a given loss or objective function. While learning rate is an important hyperparameter for most optimizers, it’s just one of several hyperparameters that can be tuned to control how the optimization process occurs.</p>
<section id="a-breakdown-of-the-main-responsibilities-of-an-optimizer" class="level2">
<h2 class="anchored" data-anchor-id="a-breakdown-of-the-main-responsibilities-of-an-optimizer">A Breakdown of The Main Responsibilities of An Optimizer:</h2>
<p>Parameter Updates: The primary role of an optimizer is to update the model’s parameters (weights and biases) in the direction that reduces the loss or error between the predicted values and the actual target values. This update typically involves computing gradients of the loss with respect to the model parameters and adjusting the parameters accordingly.</p>
<p>Learning Rate Control: Most optimizers allow you to specify or adjust the learning rate, which determines the step size of parameter updates. Choosing an appropriate learning rate is crucial, and it can impact the convergence speed and stability of training.</p>
<section id="convergence-and-stability" class="level4">
<h4 class="anchored" data-anchor-id="convergence-and-stability">Convergence and Stability:</h4>
<p>Optimizers aim to converge to a solution that minimizes the loss function while avoiding issues like getting stuck in local minima or diverging to infinity. Different optimizers use various techniques and adaptive learning rate strategies to achieve this.</p>
</section>
<section id="regularization" class="level4">
<h4 class="anchored" data-anchor-id="regularization">Regularization:</h4>
<p>Some optimizers can incorporate regularization techniques, such as L1 or L2 regularization, directly into the optimization process. This helps in preventing overfitting by adding penalty terms to the loss function.</p>
</section>
<section id="handling-sparse-data" class="level4">
<h4 class="anchored" data-anchor-id="handling-sparse-data">Handling Sparse Data:</h4>
<p>Certain optimizers, like Adagrad and Adadelta, are designed to handle sparse data efficiently by adapting learning rates individually for each parameter.</p>
</section>
<section id="choosing-initial-parameters" class="level4">
<h4 class="anchored" data-anchor-id="choosing-initial-parameters">Choosing Initial Parameters:</h4>
<p>In some cases, optimizers may be responsible for initializing model parameters. For example, the L-BFGS optimizer often requires an initial parameter estimate.</p>
</section>
<section id="hyperparameter-tuning" class="level4">
<h4 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter Tuning:</h4>
<p>While learning rate is a crucial hyperparameter, optimizers often have other hyperparameters that can be tuned, such as momentum, decay rates, or epsilon values. Tuning these hyperparameters can significantly impact training performance.</p>
</section>
</section>
<section id="types-of-optimizers" class="level2">
<h2 class="anchored" data-anchor-id="types-of-optimizers">Types of Optimizers</h2>
<p>There are several different types of optimization algorithms commonly used in machine learning and deep learning to train models. These optimizers vary in their approaches to updating model parameters during training. Here are some of the most commonly used optimizers:</p>
<section id="stochastic-gradient-descent-sgd" class="level4">
<h4 class="anchored" data-anchor-id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD):</h4>
<p>SGD is a fundamental optimization algorithm. It updates model parameters based on the gradient of the loss function with respect to those parameters. It uses a fixed learning rate.</p>
<p>Use Case: SGD is a versatile optimizer suitable for a wide range of machine learning tasks. It is often used for training deep neural networks, linear models, and support vector machines. It can be a good starting point for many optimization problems.</p>
<p>Input: Gradient of the loss function with respect to model parameters, learning rate.<br>
Output: Updated model parameters.</p>
</section>
<section id="momentum" class="level4">
<h4 class="anchored" data-anchor-id="momentum">Momentum:</h4>
<p>Momentum is an enhancement to SGD that introduces a momentum term. It accumulates gradients from previous steps to help overcome oscillations and converge faster.</p>
<p>Use Case: Momentum is beneficial for overcoming oscillations in the loss landscape that may occur when training CNNs for image classificaiton or RNNs for NLP. It is often used when training deep neural networks to accelerate convergence, especially when the loss surface has irregularities that occur when fine tuning pre-trained models for transfer learning or training VAEs.</p>
<p>Input: Gradient of the loss function with respect to model parameters, learning rate, momentum coefficient.<br>
Output: Updated model parameters.</p>
</section>
<section id="adagrad" class="level4">
<h4 class="anchored" data-anchor-id="adagrad">Adagrad:</h4>
<p>Adagrad adapts the learning rates individually for each parameter. It divides the learning rate by the square root of the sum of squared gradients for each parameter. This is useful for handling sparse data.<br>
Use Case: Adagrad is particularly useful when dealing with sparse data or when different model parameters have significantly different scales. It is commonly used in natural language processing (NLP) tasks and recommendation systems.</p>
<p>Input: Gradient of the loss function with respect to model parameters, learning rate.<br>
Output: Updated model parameters.</p>
</section>
<section id="rmsprop" class="level4">
<h4 class="anchored" data-anchor-id="rmsprop">RMSprop:</h4>
<p>RMSprop is similar to Adagrad but uses a moving average of squared gradients to adapt learning rates. It addresses some of the issues of Adagrad, such as the learning rate becoming too small.</p>
<p>Use Case: RMSprop is an adaptive learning rate method that helps mitigate the learning rate decay problem in Adagrad. It is commonly used in training recurrent neural networks (RNNs) and LSTM networks.</p>
<p>Input: Gradient of the loss function with respect to model parameters, learning rate, decay factor. Output: Updated model parameters.</p>
</section>
<section id="adam-adaptive-moment-estimation" class="level4">
<h4 class="anchored" data-anchor-id="adam-adaptive-moment-estimation">Adam (Adaptive Moment Estimation):</h4>
<p>Adam combines the ideas of momentum and RMSprop. It maintains moving averages of both gradients and their squares. Adam is known for its good performance on a wide range of tasks.</p>
<p>Use Case: Adam is a popular choice for deep learning tasks across various domains. It offers a good balance between the benefits of momentum and RMSprop. It is often used for training convolutional neural networks (CNNs) and recurrent neural networks (RNNs).</p>
<p>Input: Gradient of the loss function with respect to model parameters, learning rate, momentum coefficient, scaling decay rates. Output: Updated model parameters.</p>
</section>
<section id="adadelta" class="level4">
<h4 class="anchored" data-anchor-id="adadelta">Adadelta:</h4>
<p>Adadelta is an extension of RMSprop that seeks to address its learning rate decay problem. It uses a moving average of past gradients and past updates to adapt learning rates.</p>
<p>Use Case: Adadelta is designed to handle learning rate adaptation efficiently. It can be useful when you want to train deep learning models without manually tuning learning rates. It’s commonly used in natural language processing tasks and computer vision.</p>
<p>Input: Gradient of the loss function with respect to model parameters, moving average of past gradients, moving average of past updates. Output: Updated model parameters.</p>
<section id="nesterov-accelerated-gradient-nag" class="level5">
<h5 class="anchored" data-anchor-id="nesterov-accelerated-gradient-nag">Nesterov Accelerated Gradient (NAG):</h5>
<p>NAG is a variant of momentum that calculates the gradient slightly ahead of the current parameter values. It helps in reducing oscillations.</p>
<p>Use Case: NAG helps in reducing oscillations during training and is often used when fine-tuning pre-trained models in transfer learning scenarios. It can also be advantageous for training models with complex loss surfaces.</p>
<p>Input: Gradient of the loss function with respect to model parameters, learning rate, momentum coefficient. Output: Updated model parameters.</p>
</section>
</section>
<section id="l-bfgs-limited-memory-broyden-fletcher-goldfarb-shanno" class="level4">
<h4 class="anchored" data-anchor-id="l-bfgs-limited-memory-broyden-fletcher-goldfarb-shanno">L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno):</h4>
<p>L-BFGS is a quasi-Newton optimization method that approximates the <a href="https://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf">Hessian</a> matrix. It is often used for smaller datasets and is known for its efficiency.</p>
<p>Use Case: L-BFGS is an optimization algorithm that is well-suited for small to medium-sized datasets and when you need fast convergence. It is used in various machine learning algorithms, including logistic regression and SVMs.</p>
<p>Input: Gradient of the loss function with respect to model parameters, Hessian approximation. Output: Updated model parameters.</p>
</section>
<section id="proximal-gradient-descent" class="level4">
<h4 class="anchored" data-anchor-id="proximal-gradient-descent">Proximal Gradient Descent:</h4>
<p>This optimization algorithm is often used in sparse models and regularization. It combines gradient descent with proximal operators to enforce certain constraints.</p>
<p>Use Case: Proximal Gradient Descent is useful when dealing with sparse models and regularization techniques like L1 and L2 regularization. It’s commonly used in problems where feature selection or sparsity is essential.</p>
</section>
<section id="lbfgs-optimized-adam-l-bfgs-adam" class="level4">
<h4 class="anchored" data-anchor-id="lbfgs-optimized-adam-l-bfgs-adam">LBFGS-Optimized Adam (L-BFGS-Adam):</h4>
<p>This combines L-BFGS and Adam to leverage the benefits of both methods. It can be especially useful for deep learning models with large datasets.</p>
<p>Input: Gradient of the loss function with respect to model parameters, regularization parameter, learning rate.</p>
<p>Output: Updated model parameters.</p>
</section>
<section id="nadam" class="level4">
<h4 class="anchored" data-anchor-id="nadam">Nadam:</h4>
<p>Nadam is an extension of Adam that incorporates Nesterov momentum. It aims to combine the benefits of both Nesterov and Adam optimization techniques.</p>
</section>
<section id="ftrl-follow-the-regularized-leader" class="level4">
<h4 class="anchored" data-anchor-id="ftrl-follow-the-regularized-leader">FTRL (Follow-The-Regularized-Leader):</h4>
<p>FTRL is an online learning algorithm often used in large-scale machine learning problems. It handles sparsity and L1 regularization efficiently.</p>
<p>These are just some of the commonly used optimizers in machine learning and deep learning. The choice of optimizer can significantly impact the training process and the final performance of a model. The selection often depends on the specific problem, architecture, and dataset being used.</p>
<p>In summary, while learning rate is a vital aspect of optimization, optimizers play a broader role in controlling the training process and updating model parameters. They are responsible for guiding the model towards finding optimal parameter values that minimize the loss function and achieve better generalization on unseen data.</p>
</section>
<section id="references" class="level4">
<h4 class="anchored" data-anchor-id="references">References:</h4>
<p><a href="https://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf">Deep learning via Hessian-free optimization</a></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>