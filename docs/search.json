[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thank you for stopping by. This blog is my way of documenting my journey through the second semester of part 2 of the FastAI: Machine Learning for Coders\nI’ve had a successful career as a product leader and owner at amazing companies like Microsoft, Blackberry, and Lucent. I hold a bachelor’s in Physics and a Master’s in Computer Science. Some of my more notable accomplishments were the first to launch a Garmin GPS-integrated smartphone; the 3600 Curve from Blackberry and the first to launch the 920 Lumia Windows Phone with Microsoft. I’ve had the privilege of supporting product, technical sales and business operations teams leading software and hardware launches. My interest in machine learning started from working with numpy, pandas, and SQLAlchemy as a Developer Evangelist Lead at a Geo-Spatial and Analytics company. My curiosity led me to the courses at FastAI where after a couple of false starts I joined a study group as a way to learn and interact with others who share a similar interest in machine learning. The combination of my fondness for linear algebra and diff equations, accessibility of Python as a language, and last three years working as a Developer Evangelist at a GeoSpatial and Analytics Company is what motivated me to go through the FastAI course."
  },
  {
    "objectID": "posts/Resources and Links/index.html",
    "href": "posts/Resources and Links/index.html",
    "title": "Resources and Links",
    "section": "",
    "text": "I’ll attempt to keep the content and links as current as possible.\n\n\n\nStuttgart City Library\n\n\n\nMachine Learning Resources:\nFastAI\nMeta AI\nHuggingface\nPyTorch\nPapers With Code\nQuarto"
  },
  {
    "objectID": "posts/2023-08-28-learning-rate-finder.html",
    "href": "posts/2023-08-28-learning-rate-finder.html",
    "title": "Learning Rate Finder and Annealing",
    "section": "",
    "text": "Relativaty by MC Escher"
  },
  {
    "objectID": "posts/2023-08-28-learning-rate-finder.html#introduction",
    "href": "posts/2023-08-28-learning-rate-finder.html#introduction",
    "title": "Learning Rate Finder and Annealing",
    "section": "Introduction",
    "text": "Introduction\nSimulated annealing is a global optimization technique that has been widely used in various fields, including physics, chemistry, and engineering. It is particularly useful when dealing with complex systems where traditional optimization methods may fail to converge or produce suboptimal solutions. In machine learning, simulated annealing can be applied to optimize hyperparameters of models, leading to better model performance and generalization.\nIn this article, we will explore how to use simulated annealing in machine learning using FastAI’s libraries. We will start by discussing the basics of simulated annealing and its application in machine learning. Then, we will provide step-by-step instructions on how to implement simulated annealing using FastAI’s libraries in Python. Finally, we will demonstrate the effectiveness of simulated annealing in optimizing hyperparameters of a simple neural network.\nYou can learn more about the amazing work FastAI does in pedogogy of machine learning while makeing signifant contributions to AI at Fast.AI. I encourage you to learn more about what Jeremy is upto and as a current student, join me on this learning journey by goin to Practical Deep Learning for Coders.\n\nWhat is Simulated Annealing?\nSimulated annealing is a stochastic optimization algorithm inspired by the process of annealing in metallurgy. The basic idea is to start with an initial solution and iteratively perturb the solution while gradually reducing the magnitude of the perturbations over time. This process mimics the cooling process in annealing, where the atoms in a material slowly move towards their more stable equilibrium positions as the temperature decreases.\nThe key feature of simulated annealing is the “annealing schedule,” which controls the rate at which the perturbation size is reduced during the optimization process. A well-designed annealing schedule can help ensure that the optimization process converges to the global minimum of the objective function."
  },
  {
    "objectID": "posts/2023-08-28-learning-rate-finder.html#applications-of-simulated-annealing-in-machine-learning",
    "href": "posts/2023-08-28-learning-rate-finder.html#applications-of-simulated-annealing-in-machine-learning",
    "title": "Learning Rate Finder and Annealing",
    "section": "Applications of Simulated Annealing in Machine Learning",
    "text": "Applications of Simulated Annealing in Machine Learning\nSimulated annealing has several applications in machine learning, including:\n\nHyperparameter Optimization\nOne of the most common applications of simulated annealing in machine learning is hyperparameter optimization. Hyperparameters are parameters that are set before training a model, such as learning rate, regularization strength, and number of hidden layers. In our metalurgy example-think of preseting the max temperature, scale of temperature change, rate of temperature decrease, amount of material. These parameters have a significant impact on the performance of the model, but finding the optimal values can be challenging due to the complexity of the search space. Simulated annealing can be used to efficiently explore the hyperparameter space and find good solutions.\n\n\nNeural Network Architecture Search\nAnother application of simulated annealing in machine learning is neural network architecture search. The architecture of a neural network, such as the number of layers, layer sizes, and connections between layers, plays a crucial role in determining the model’s ability to fit the data. Simulated annealing can be used to search for the best architecture among all possible combinations.\n\n\nModel Selection\nSimulated annealing can also be used for model selection, where the goal is to choose the best model from a set of candidate models. Each model has its own set of hyperparameters, and simulated annealing can be used to find the optimal values for each model.\n\n\nSimulated Annealing Algorithms Built into Pytorch\nPyTorch provides several built-in functions for performing annealing during training. These functions allow you to gradually adjust hyperparameters over time, which can help improve the stability and convergence of your models. Some commonly used annealing functions in PyTorch include:\n\ntorch.optim.lr_scheduler.StepLR:\nThis scheduler reduces the learning rate of each parameter group by a factor at each step. You specify the reduction factor and the interval between steps. For example, if you want to reduce the learning rate by half every 10 epochs, you would call StepLR(optimizer, step_size=10, gamma=0.5). ###### torch.optim.lr_scheduler.MultiStepLR:\nSimilar to StepLR, but allows you to specify multiple milestone steps at which the learning rate should be reduced. For example, if you want to reduce the learning rate by half after 10 epochs and then again after 20 epochs, you would call MultiStepLR(optimizer, milestones=[10, 20], gamma=0.5). ###### torch.optim.lr_scheduler.ExponentialLR:\nReduces the learning rate exponentially based on a fixed schedule. You specify the decay rate and the interval between updates. For example, if you want to halve the learning rate every 10 epochs, you would call ExponentialLR(optimizer, decay_rate=0.9, update_interval=10). ##### torch.optim.lr_scheduler.CosineAnnealingLR:\nGradually reduces the learning rate over a specified number of iterations. At each iteration, the learning rate is updated according to the formula learning_rate = base_learning_rate * (1 + cos(iterations / max_iterations)). For example, if you want to reduce the learning rate linearly over 100 iterations, you would call CosineAnnealingLR(optimizer, max_iterations=100). ###### torch.optim.lr_scheduler.ReduceLROnPlateau:\nReduces the learning rate when a metric stops improving. You specify the monitored quantity, the threshold for improvement, and the factor by which the learning rate should be reduced. For example, if you want to reduce the learning rate by half when validation loss fails to improve for 10 consecutive epochs, you would call ReduceLROnPlateau(optimizer, mode=‘min’, factor=0.5, patience=10).\nThese are just a few examples of the annealing functions available in PyTorch. There are also other customizable options, such as torch.optim.lr_scheduler.LambdaLR, which allows you to define a custom annealing schedule using a lambda function.\n\n\n\nHow to Implement Simulated Annealing in FastAI\nThe FastAI framework is highly integrated with the PyTorch library allowing you to incorporate many of the models and functions and learn more about FastAI by going to their documents page.\nStep 1: Install FastAI using the instructions: You can use the FastAI library directly from google Colab or First, choose your go forward method of installation via Git, Conda, or pip:\nFastAI From a jupyter or colab notebook environment you can install them directly by typing: !pip install fastai\nStep 2: Import Libraries We will use NumPy for array operations and FastAI’s optimize library for implementing simulated annealing.\nimport numpy as np\nfrom fastai.optimize import *\nStep 3: Define Objective Function Define the objective function that you want to minimize. For example, let’s consider a simple neural network with one input layer, one output layer, and no hidden layers. The objective function could be the mean squared error (MSE) between the predicted outputs and the true labels.\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\nHere, TrainLearner is a custom class that defines the neural network architecture, and predict() and get_loss() are functions that perform forward pass and backward pass through the network, respectively.\nStep 4: Define Annealing Schedule Next, define the annealing schedule. The annealing schedule should specify the starting temperature, ending temperature, and the reduction factor for each iteration. Here’s an example:\nstart_temp = 1000\nend_temp = 1e-6\nreduction_factor = 0.95\nschedule = np.linspace(start_temp, end_temp, num_iterations) ** reduction_factor\nThis schedule starts with a high temperature (start_temp) and reduces it exponentially until reaching a low temperature (end_temp). The reduction factor (reduction_factor) controls the rate at which the temperature is reduced.\nStep 5: Run Simulated Annealing Finally, run the simulated annealing algorithm. Here’s some sample code:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Learning Mechanic",
    "section": "",
    "text": "Learner\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCosine Annealing With a FastAI Learner\n\n\n\n\n\nImplementing a Cosine Annealer with the FastAI learner\n\n\n\n\n\n\nSep 5, 2023\n\n\nManuel Pardo\n\n\n\n\n\n\n  \n\n\n\n\nResources and Links\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nManuel Pardo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-08-28- FastAI Cosine Annealer.html",
    "href": "posts/2023-08-28- FastAI Cosine Annealer.html",
    "title": "Cosine Annealing With a FastAI Learner",
    "section": "",
    "text": "Relativity by MC Escher"
  },
  {
    "objectID": "posts/2023-08-28- FastAI Cosine Annealer.html#cosine-annealing",
    "href": "posts/2023-08-28- FastAI Cosine Annealer.html#cosine-annealing",
    "title": "Cosine Annealing With a FastAI Learner",
    "section": "Cosine Annealing",
    "text": "Cosine Annealing\nOne popular variant of simulated annealing is cosine annealing. Instead of decreasing the learning rate linearly over time, cosine annealing uses a cosine function to gradually reduce the learning rate. This allows the model to slow down its descent into the optimum and helps prevent getting stuck in local minima.\nHere’s the formula for cosine annealing:\n\\[\nCurrentLF = StartingRate * (Maxixum(cos(pi * (1 - CurrentSteps/TotalSteps)))\n\\]\nwhere StartingRate is the initial learning rate, CurrentLF is the learning rate calcuated at the CurrentSteps, and TotalSteps is the total number of steps."
  },
  {
    "objectID": "posts/2023-08-28- FastAI Cosine Annealer.html#implementing-cosine-annealing-in-fastai",
    "href": "posts/2023-08-28- FastAI Cosine Annealer.html#implementing-cosine-annealing-in-fastai",
    "title": "Cosine Annealing With a FastAI Learner",
    "section": "Implementing Cosine Annealing in FastAI",
    "text": "Implementing Cosine Annealing in FastAI\nTo incorporate cosine annealing into our FastAI workflow, we’ll extend the LRFinder class and add a new method called cosine_annealing. Here’s the updated code:\nclass LRFinderCB(Callback):\n    def __init__(self, lr_mult=1.3):\n        super().__init__()\n        self.lr_mult = lr_mult\n        self.cosine_annealing = False \n\n    def before_fit(self, learner):\n        self.lrs, self.losses = [], []\n        self.min = math.inf\n\n    def after_batch(self, learner):\n        if not learner.training:\n            raise CancelEpochException()\n\n        self.lrs.append(learner.opt.param_groups[0]['lr'])\n        loss = to_cpu(learner.loss)\n        self.losses.append(loss)\n        if loss &lt; self.min:\n            self.min = loss\n\n        if self.cosine_annealing:\n            learning_rate = self.lr_mult * max(cos(pi * (1 - len(self.lrs) / len(self.losses))), 0.5)\n            for g in learner.opt.param_groups:\n                g['lr'] = learning_rate\n\nclass Metric:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.vals, self.ns = [], []\n\n    def add(self, inp, targ=None, n=1):\n        self.last = self.calc(inp, targ)\n        self.vals.append(self.last)\n        self.ns.append(n)\n\n    @property\n    def value(self):\n        ns = tensor(self.ns)\n        return (tensor(self.vals) * ns).sum() / ns.sum()\n\n    def calc(self, inps, targs):\n        return (inps == targs).float().mean()\n\nclass Accuracy(Metric):\n    def calc(self, inps, targs):\n        return (inps == targs).float().mean()\nNow, when we initialize the LRFinderCB object, we can pass cosine_annealing=True to enable cosine annealing. The cosine_annealing method will update the learning rate according to the cosine annealing formula."
  },
  {
    "objectID": "posts/2023-08-28- FastAI Cosine Annealer.html#using-the-metric-class-to-calculate-accuracy",
    "href": "posts/2023-08-28- FastAI Cosine Annealer.html#using-the-metric-class-to-calculate-accuracy",
    "title": "Cosine Annealing With a FastAI Learner",
    "section": "Using the Metric Class to Calculate Accuracy",
    "text": "Using the Metric Class to Calculate Accuracy\nTo calculate accuracy during training, we can use the Metric class provided by FastAI. This class allows us to compute a metric over a dataset and print it out at each epoch.\nWe’ll create a custom accuracy metric that calculates the accuracy of our model on the validation set. Here’s how to do it:\nfrom fastai import metrics\nclass Accuracy(metrics.Metric):\n    def __init__(self):\n        super().__init__()\n        self.correct = 0\n        self.total = 0\n\n    def add(self, pred, true):\n        self.correct += (pred == true).sum().item()\n        self.total += pred.size(0)\n\n    def value(self):\n        return self.correct / self.total\nThis metric class takes in two tensors, pred and true, which represent the predicted outputs and the true labels, respectively. It then computes the accuracy by counting the number of correctly predicted samples and dividing it by the total number of samples.\nWe can now register this metric with FastAI’s CallbackList to get the accuracy at each epoch:\nfrom fastai.callbacks import CallbackList\n\ncb_list = CallbackList()\ncb_list.insert(Accuracy())\nWith this callback list, FastAI will call the Accuracy metric at each epoch and print out the accuracy."
  },
  {
    "objectID": "posts/2023-08-28- FastAI Cosine Annealer.html#putting-everything-together",
    "href": "posts/2023-08-28- FastAI Cosine Annealer.html#putting-everything-together",
    "title": "Cosine Annealing With a FastAI Learner",
    "section": "Putting Everything Together",
    "text": "Putting Everything Together\nNow that we have all the necessary components, let’s put them together to create a complete FastAI training loop with cosine annealing and accuracy calculation:\nfrom fastai import TrainLoop\n\ntrain_loop = TrainLoop(model=model,\n                     dataloader=dataloader,\n                     optimizer=optimizer,\n                     loss_fn=loss_fn,\n                     metrics=[Accuracy()],\n                     callbacks=[cb_list],\n                     device=\"cuda\")\ntrain_loop.train(num_epochs=10)\nThis training loop will train the model for 10 epochs, computing the accuracy at each epoch using the Accuracy metric and updating the learning rate using cosine annealing.\nAnd that’s it! With these few lines of code, you’ve implemented a powerful training loop that leverages the flexibility and ease of use of FastAI.\nConclusion In this tutorial, we learned how to implement cosine annealing and accuracy calculation in a FastAI training loop. By extending the LRFinder class and creating a custom Accuracy metric, we were able to create a complete training loop that adapts the learning rate during training and prints out the accuracy at each epoch.\nWith this knowledge, you can now apply these techniques to your own deep learning projects and improve the performance of your models. Happy coding!"
  },
  {
    "objectID": "posts/2023-08-28- FastAI Cosine Annealer.html#conclusion",
    "href": "posts/2023-08-28- FastAI Cosine Annealer.html#conclusion",
    "title": "Cosine Annealing With a FastAI Learner",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we learned how to implement cosine annealing and accuracy calculation in a FastAI training loop. By extending the LRFinder class and creating a custom Accuracy metric, we were able to create a complete training loop that adapts the learning rate during training and prints out the accuracy at each epoch.\nWith this knowledge, you can now apply these techniques to your own deep-learning projects and improve the performance of your models. Happy coding!"
  },
  {
    "objectID": "posts/FastAIlearner.html",
    "href": "posts/FastAIlearner.html",
    "title": "Learner",
    "section": "",
    "text": "::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=6}\n:::\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom contextlib import contextmanager\nfrom torch import nn,tensor\nfrom datasets import load_dataset,load_dataset_builder\nfrom miniai.datasets import *\nfrom miniai.conv import *\nimport logging\nfrom fastcore.test import test_close\nimport numpy as np\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\nlogging.disable(logging.WARNING)\nx,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\nbs = 512\nlr = 0.2\nm,nh = 28*28,50\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\ndef get_model(): return nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([512, 784]), tensor([2, 8, 7, 3, 0, 3, 2, 8, 6, 9]))"
  },
  {
    "objectID": "posts/FastAIlearner.html#basic-callbacks-learner",
    "href": "posts/FastAIlearner.html#basic-callbacks-learner",
    "title": "Learner",
    "section": "Basic Callbacks Learner",
    "text": "Basic Callbacks Learner\n\nclass CancelFitException(Exception): pass\nclass CancelBatchException(Exception): pass\nclass CancelEpochException(Exception): pass\n\n\nclass Callback(): order = 0\n\n\ndef run_cbs(cbs, method_nm, learn=None):\n    for cb in sorted(cbs, key=attrgetter('order')):\n        method = getattr(cb, method_nm, None)\n        if method is not None: method(learn)"
  },
  {
    "objectID": "posts/FastAIlearner.html#metrics",
    "href": "posts/FastAIlearner.html#metrics",
    "title": "Learner",
    "section": "Metrics",
    "text": "Metrics\n\nclass Metric:\n    def __init__(self): self.reset()\n    def reset(self): self.vals,self.ns = [],[]\n    def add(self, inp, targ=None, n=1):\n        self.last = self.calc(inp, targ)\n        self.vals.append(self.last)\n        self.ns.append(n)\n    @property\n    def value(self):\n        ns = tensor(self.ns)\n        return (tensor(self.vals)*ns).sum()/ns.sum()\n    def calc(self, inps, targs): return inps\n    \n\n\nclass Accuracy(Metric):\n    def calc(self, inps, targs): return (inps==targs).float().mean()"
  },
  {
    "objectID": "posts/FastAIlearner.html#some-callbacks",
    "href": "posts/FastAIlearner.html#some-callbacks",
    "title": "Learner",
    "section": "Some callbacks",
    "text": "Some callbacks\npip install torcheval\n\nfrom torcheval.metrics import MulticlassAccuracy,Mean\n\n\ndef to_cpu(x):\n    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n    if isinstance(x, list): return [to_cpu(o) for o in x]\n    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n    res = x.detach().cpu()\n    return res.float() if res.dtype==torch.float16 else res\n\n\nclass MetricsCB(Callback):\n    def __init__(self, *ms, **metrics):\n        for o in ms: metrics[type(o).__name__] = o\n        self.metrics = metrics\n        self.all_metrics = copy(metrics)\n        self.all_metrics['loss'] = self.loss = Mean()\n\n    def _log(self, d): print(d)\n    def before_fit(self, learn): learn.metrics = self\n    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n\n    def after_epoch(self, learn):\n        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n        log['epoch'] = learn.epoch\n        log['train'] = 'train' if learn.model.training else 'eval'\n        self._log(log)\n\n    def after_batch(self, learn):\n        x,y,*_ = to_cpu(learn.batch)\n        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n        self.loss.update(to_cpu(learn.loss), weight=len(x))\n\n\nclass DeviceCB(Callback):\n    def __init__(self, device=def_device): fc.store_attr()\n    def before_fit(self, learn):\n        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)"
  },
  {
    "objectID": "posts/FastAIlearner.html#flexible-learner",
    "href": "posts/FastAIlearner.html#flexible-learner",
    "title": "Learner",
    "section": "Flexible learner",
    "text": "Flexible learner\n\nclass TrainCB(Callback):\n    def __init__(self, n_inp=1): self.n_inp = n_inp\n    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])\n    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])\n    def backward(self, learn): learn.loss.backward()\n    def step(self, learn): learn.opt.step()\n    def zero_grad(self, learn): learn.opt.zero_grad()\n\nNB: I added self.n_inp after the lesson. This allows us to train models with more than one input or output.\n\nclass ProgressCB(Callback):\n    order = MetricsCB.order+1\n    def __init__(self, plot=False): self.plot = plot\n    def before_fit(self, learn):\n        learn.epochs = self.mbar = master_bar(learn.epochs)\n        self.first = True\n        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n        self.losses = []\n        self.val_losses = []\n\n    def _log(self, d):\n        if self.first:\n            self.mbar.write(list(d), table=True)\n            self.first = False\n        self.mbar.write(list(d.values()), table=True)\n\n    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if self.plot and hasattr(learn, 'metrics') and learn.training:\n            self.losses.append(learn.loss.item())\n            if self.val_losses: self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n    \n    def after_epoch(self, learn): \n        if not learn.training:\n            if self.plot and hasattr(learn, 'metrics'): \n                self.val_losses.append(learn.metrics.all_metrics['loss'].compute())\n                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n\nNB: Added validation loss plotting after the lesson.\n\nbs = 1014\nlr = 0.9\nm,nh = 28*28,100\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1014, 784]), tensor([8, 8, 5, 0, 6, 2, 5, 1, 8, 4]))\n\n\n\nclass with_cbs:\n    def __init__(self, nm): self.nm = nm\n    def __call__(self, f):\n        def _f(o, *args, **kwargs):\n            try:\n                o.callback(f'before_{self.nm}')\n                f(o, *args, **kwargs)\n                o.callback(f'after_{self.nm}')\n            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n            finally: o.callback(f'cleanup_{self.nm}')\n        return _f\n\n\nclass Learner():\n    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n        cbs = fc.L(cbs)\n        fc.store_attr()\n\n    @with_cbs('batch')\n    def _one_batch(self):\n        self.predict()\n        self.callback('after_predict')\n        self.get_loss()\n        self.callback('after_loss')\n        if self.training:\n            self.backward()\n            self.callback('after_backward')\n            self.step()\n            self.callback('after_step')\n            self.zero_grad()\n\n    @with_cbs('epoch')\n    def _one_epoch(self):\n        for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n\n    def one_epoch(self, training):\n        self.model.train(training)\n        self.dl = self.dls.train if training else self.dls.valid\n        self._one_epoch()\n\n    @with_cbs('fit')\n    def _fit(self, train, valid):\n        for self.epoch in self.epochs:\n            if train: self.one_epoch(True)\n            if valid: torch.no_grad()(self.one_epoch)(False)\n\n    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs: self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            if lr is None: lr = self.lr\n            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n            self._fit(train, valid)\n        finally:\n            for cb in cbs: self.cbs.remove(cb)\n\n    def __getattr__(self, name):\n        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n        raise AttributeError(name)\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n    \n    @property\n    def training(self): return self.model.training"
  },
  {
    "objectID": "posts/FastAIlearner.html#updated-versions-since-the-lesson",
    "href": "posts/FastAIlearner.html#updated-versions-since-the-lesson",
    "title": "Learner",
    "section": "Updated versions since the lesson",
    "text": "Updated versions since the lesson\nAfter the lesson we noticed that contextlib.context_manager has a surprising “feature” which doesn’t let us raise an exception before the yield. Therefore we’ve replaced the context manager with a decorator in this updated version of Learner. We have also added a few more callbacks in one_epoch().\n\nclass with_cbs:\n    def __init__(self, nm): self.nm = nm\n    def __call__(self, f):\n        def _f(o, *args, **kwargs):\n            try:\n                o.callback(f'before_{self.nm}')\n                f(o, *args, **kwargs)\n                o.callback(f'after_{self.nm}')\n            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n            finally: o.callback(f'cleanup_{self.nm}')\n        return _f\n\n\nclass Learner():\n    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n        cbs = fc.L(cbs)\n        fc.store_attr()\n\n    @with_cbs('batch')\n    def _one_batch(self):\n        self.predict()\n        self.callback('after_predict')\n        self.get_loss()\n        self.callback('after_loss')\n        if self.training:\n            self.backward()\n            self.callback('after_backward')\n            self.step()\n            self.callback('after_step')\n            self.zero_grad()\n\n    @with_cbs('epoch')\n    def _one_epoch(self):\n        for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n\n    def one_epoch(self, training):\n        self.model.train(training)\n        self.dl = self.dls.train if training else self.dls.valid\n        self._one_epoch()\n\n    @with_cbs('fit')\n    def _fit(self, train, valid):\n        for self.epoch in self.epochs:\n            if train: self.one_epoch(True)\n            if valid: torch.no_grad()(self.one_epoch)(False)\n\n    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs: self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            if lr is None: lr = self.lr\n            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n            self._fit(train, valid)\n        finally:\n            for cb in cbs: self.cbs.remove(cb)\n\n    def __getattr__(self, name):\n        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n        raise AttributeError(name)\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n    \n    @property\n    def training(self): return self.model.training"
  },
  {
    "objectID": "posts/FastAIlearner.html#trainlearner-and-momentumlearner",
    "href": "posts/FastAIlearner.html#trainlearner-and-momentumlearner",
    "title": "Learner",
    "section": "TrainLearner and MomentumLearner",
    "text": "TrainLearner and MomentumLearner\n\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n\n\nclass MomentumLearner(TrainLearner):\n    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):\n        self.mom = mom\n        super().__init__(model, dls, loss_func, lr, cbs, opt_func)\n\n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.model.parameters(): p.grad *= self.mom\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=0.1, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.693\n0.916\n0\ntrain\n\n\n0.785\n0.586\n0\neval"
  },
  {
    "objectID": "posts/FastAIlearner.html#lrfindercb",
    "href": "posts/FastAIlearner.html#lrfindercb",
    "title": "Learner",
    "section": "LRFinderCB",
    "text": "LRFinderCB\n\nclass CosAnnealCB(Callback):\n    def __init__(self, lr_mult=1.3): \n        fc.store_attr()\n \n    def before_fit(self, learn):\n        self.epochs, self.lrs,self.losses = [],[], []\n        self.min = math.inf\n        self.t_iter = len(learn.dls.train) * learn.n_epochs\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        c_iter = learn.iter\n        self.losses.append(loss)\n        self.epochs.append(c_iter)\n        if loss &lt; self.min: self.min = loss\n        if loss &gt; self.min*2: raise CancelFitException()\n        for g in learn.opt.param_groups: g['lr'] *= self.lr_mult\n        g['lr'] = g['lr']*max(np.cos((1-4.0*np.pi*(c_iter / self.t_iter))),1.5)\n        \n\n\nlrfind = CosAnnealCB()\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), lrfind]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-6, cbs=cbs)\nlearn.fit(1)\n\n\nplt.plot(lrfind.epochs, lrfind.lrs)\nplt.xlabel('Epochs')\nplt.ylabel('Learning Rate (Cosine)')\nplt.xscale('linear')\n\n\n\n\nFig 3 - Learning Rate Cosine Annealer\n\n\n\n\n\nplt.plot(lrfind.epochs, lrfind.losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xscale('linear')\n\n\n\n\nFig 4 - Learning Rate Cosine Annealer\n\n\n\n\n\nclass LRFinderCB(Callback):\n    def __init__(self, lr_mult=1.3): \n        fc.store_attr()\n \n    def before_fit(self, learn):\n        self.epochs, self.lrs,self.losses = [],[], []\n        self.min = math.inf\n        self.t_iter = len(learn.dls.train) * learn.n_epochs\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        c_iter = learn.iter\n        self.losses.append(loss)\n        self.epochs.append(c_iter)\n        if loss &lt; self.min: self.min = loss\n        if loss &gt; self.min*2: raise CancelFitException()\n        for g in learn.opt.param_groups: g['lr'] *= self.lr_mult\n\n\nlrfind = LRFinderCB()\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), lrfind]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-6, cbs=cbs)\nlearn.fit(1)\n\n\nplt.plot(lrfind.epochs, lrfind.lrs)\nplt.xlabel('Epochs')\nplt.ylabel('Learning Rate')\nplt.xscale('linear')\n\n\n\n\nFig 1 - Learning Rate Finder\n\n\n\n\n\nplt.plot(lrfind.epochs, lrfind.losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xscale('linear')\n\n\n\n\nFig 2 - Learning Rate Cosine Annealer"
  }
]