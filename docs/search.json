[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thank you for stopping by. This blog is my way of documenting my journey through the second semester of part 2 of the FastAI: Machine Learning for Coders\nI’ve had a successful career as a product leader and owner at amazing companies like Microsoft, Blackberry, and Lucent. I hold a bachelor’s in Physics and a Master’s in Computer Science. Some of my more notable accomplishments were the first to launch a Garmin GPS-integrated smartphone; the 3600 Curve from Blackberry and the first to launch the 920 Lumia Windows Phone with Microsoft. I’ve had the privilege of supporting product, technical sales and business operations teams leading software and hardware launches. My interest in machine learning started from working with numpy, pandas, and SQLAlchemy as a Developer Evangelist Lead at a Geo-Spatial and Analytics company. My curiosity led me to the courses at FastAI where after a couple of false starts I joined a study group as a way to learn and interact with others who share a similar interest in machine learning. The combination of my fondness for linear algebra and diff equations, accessibility of Python as a language, and last three years working as a Developer Evangelist at a GeoSpatial and Analytics Company is what motivated me to go through the FastAI course."
  },
  {
    "objectID": "posts/Resources and Links/index.html",
    "href": "posts/Resources and Links/index.html",
    "title": "Resources and Links",
    "section": "",
    "text": "I’ll attempt to keep the content and links as current as possible.\n\n\n\nStuttgart City Library\n\n\n\nMachine Learning Resources:\nFastAI\nMeta AI\nHuggingface\nPyTorch\nPapers With Code\nQuarto"
  },
  {
    "objectID": "posts/2023-08-28- FastAI Cosine Annealer/2023-08-28- FastAI Cosine Annealer.html",
    "href": "posts/2023-08-28- FastAI Cosine Annealer/2023-08-28- FastAI Cosine Annealer.html",
    "title": "Cosine Annealing With a FastAI Learner",
    "section": "",
    "text": "["
  },
  {
    "objectID": "posts/2023-08-28- FastAI Cosine Annealer/2023-08-28- FastAI Cosine Annealer.html#conclusion",
    "href": "posts/2023-08-28- FastAI Cosine Annealer/2023-08-28- FastAI Cosine Annealer.html#conclusion",
    "title": "Cosine Annealing With a FastAI Learner",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we learned how to implement cosine annealing and accuracy calculation in a FastAI training loop. By extending the LRFinder class and creating a custom Accuracy metric, we were able to create a complete training loop that adapts the learning rate during training and prints out the accuracy at each epoch.\nWith this knowledge, you can now apply these techniques to your own deep-learning projects and improve the performance of your models."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Learning Mechanic",
    "section": "",
    "text": "Study Notes-Optimizers and Schedulers\n\n\n\n\n\n\n\nStudy Notes\n\n\nOptimizers\n\n\nSchedulers\n\n\n\n\nA Highlevel look at Optimizers and Schedulers\n\n\n\n\n\n\nSep 20, 2023\n\n\nManuel Pardo\n\n\n\n\n\n\n  \n\n\n\n\nCosine Annealing With a FastAI Learner\n\n\n\n\n\nImplementing a Cosine Annealer with the FastAI learner\n\n\n\n\n\n\nSep 5, 2023\n\n\nManuel Pardo\n\n\n\n\n\n\n  \n\n\n\n\nResources and Links\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nManuel Pardo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Resources and Links/Notes.html",
    "href": "posts/Resources and Links/Notes.html",
    "title": "Study Notes-Optimizers and Schedulers",
    "section": "",
    "text": "In machine learning and deep learning, an optimizer’s primary role is to update model parameters during training in order to minimize a given loss or objective function. While learning rate is an important hyperparameter for most optimizers, it’s just one of several hyperparameters that can be tuned to control how the optimization process occurs."
  },
  {
    "objectID": "posts/Resources and Links/Notes.html#a-breakdown-of-the-main-responsibilities-of-an-optimizer",
    "href": "posts/Resources and Links/Notes.html#a-breakdown-of-the-main-responsibilities-of-an-optimizer",
    "title": "Study Notes-Optimizers and Schedulers",
    "section": "A Breakdown of The Main Responsibilities of An Optimizer:",
    "text": "A Breakdown of The Main Responsibilities of An Optimizer:\nParameter Updates: The primary role of an optimizer is to update the model’s parameters (weights and biases) in the direction that reduces the loss or error between the predicted values and the actual target values. This update typically involves computing gradients of the loss with respect to the model parameters and adjusting the parameters accordingly.\nLearning Rate Control: Most optimizers allow you to specify or adjust the learning rate, which determines the step size of parameter updates. Choosing an appropriate learning rate is crucial, and it can impact the convergence speed and stability of training.\n\nConvergence and Stability:\nOptimizers aim to converge to a solution that minimizes the loss function while avoiding issues like getting stuck in local minima or diverging to infinity. Different optimizers use various techniques and adaptive learning rate strategies to achieve this.\n\n\nRegularization:\nSome optimizers can incorporate regularization techniques, such as L1 or L2 regularization, directly into the optimization process. This helps in preventing overfitting by adding penalty terms to the loss function.\n\n\nHandling Sparse Data:\nCertain optimizers, like Adagrad and Adadelta, are designed to handle sparse data efficiently by adapting learning rates individually for each parameter.\n\n\nChoosing Initial Parameters:\nIn some cases, optimizers may be responsible for initializing model parameters. For example, the L-BFGS optimizer often requires an initial parameter estimate.\n\n\nHyperparameter Tuning:\nWhile learning rate is a crucial hyperparameter, optimizers often have other hyperparameters that can be tuned, such as momentum, decay rates, or epsilon values. Tuning these hyperparameters can significantly impact training performance."
  },
  {
    "objectID": "posts/Resources and Links/Notes.html#types-of-optimizers",
    "href": "posts/Resources and Links/Notes.html#types-of-optimizers",
    "title": "Study Notes-Optimizers and Schedulers",
    "section": "Types of Optimizers",
    "text": "Types of Optimizers\nThere are several different types of optimization algorithms commonly used in machine learning and deep learning to train models. These optimizers vary in their approaches to updating model parameters during training. Here are some of the most commonly used optimizers:\n\nStochastic Gradient Descent (SGD):\nSGD is a fundamental optimization algorithm. It updates model parameters based on the gradient of the loss function with respect to those parameters. It uses a fixed learning rate.\nUse Case: SGD is a versatile optimizer suitable for a wide range of machine learning tasks. It is often used for training deep neural networks, linear models, and support vector machines. It can be a good starting point for many optimization problems.\nInput: Gradient of the loss function with respect to model parameters, learning rate.\nOutput: Updated model parameters.\n\n\nMomentum:\nMomentum is an enhancement to SGD that introduces a momentum term. It accumulates gradients from previous steps to help overcome oscillations and converge faster.\nUse Case: Momentum is beneficial for overcoming oscillations in the loss landscape that may occur when training CNNs for image classificaiton or RNNs for NLP. It is often used when training deep neural networks to accelerate convergence, especially when the loss surface has irregularities that occur when fine tuning pre-trained models for transfer learning or training VAEs.\nInput: Gradient of the loss function with respect to model parameters, learning rate, momentum coefficient.\nOutput: Updated model parameters.\n\n\nAdagrad:\nAdagrad adapts the learning rates individually for each parameter. It divides the learning rate by the square root of the sum of squared gradients for each parameter. This is useful for handling sparse data.\nUse Case: Adagrad is particularly useful when dealing with sparse data or when different model parameters have significantly different scales. It is commonly used in natural language processing (NLP) tasks and recommendation systems.\nInput: Gradient of the loss function with respect to model parameters, learning rate.\nOutput: Updated model parameters.\n\n\nRMSprop:\nRMSprop is similar to Adagrad but uses a moving average of squared gradients to adapt learning rates. It addresses some of the issues of Adagrad, such as the learning rate becoming too small.\nUse Case: RMSprop is an adaptive learning rate method that helps mitigate the learning rate decay problem in Adagrad. It is commonly used in training recurrent neural networks (RNNs) and LSTM networks.\nInput: Gradient of the loss function with respect to model parameters, learning rate, decay factor. Output: Updated model parameters.\n\n\nAdam (Adaptive Moment Estimation):\nAdam combines the ideas of momentum and RMSprop. It maintains moving averages of both gradients and their squares. Adam is known for its good performance on a wide range of tasks.\nUse Case: Adam is a popular choice for deep learning tasks across various domains. It offers a good balance between the benefits of momentum and RMSprop. It is often used for training convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\nInput: Gradient of the loss function with respect to model parameters, learning rate, momentum coefficient, scaling decay rates. Output: Updated model parameters.\n\n\nAdadelta:\nAdadelta is an extension of RMSprop that seeks to address its learning rate decay problem. It uses a moving average of past gradients and past updates to adapt learning rates.\nUse Case: Adadelta is designed to handle learning rate adaptation efficiently. It can be useful when you want to train deep learning models without manually tuning learning rates. It’s commonly used in natural language processing tasks and computer vision.\nInput: Gradient of the loss function with respect to model parameters, moving average of past gradients, moving average of past updates. Output: Updated model parameters.\n\nNesterov Accelerated Gradient (NAG):\nNAG is a variant of momentum that calculates the gradient slightly ahead of the current parameter values. It helps in reducing oscillations.\nUse Case: NAG helps in reducing oscillations during training and is often used when fine-tuning pre-trained models in transfer learning scenarios. It can also be advantageous for training models with complex loss surfaces.\nInput: Gradient of the loss function with respect to model parameters, learning rate, momentum coefficient. Output: Updated model parameters.\n\n\n\nL-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno):\nL-BFGS is a quasi-Newton optimization method that approximates the Hessian matrix. It is often used for smaller datasets and is known for its efficiency.\nUse Case: L-BFGS is an optimization algorithm that is well-suited for small to medium-sized datasets and when you need fast convergence. It is used in various machine learning algorithms, including logistic regression and SVMs.\nInput: Gradient of the loss function with respect to model parameters, Hessian approximation. Output: Updated model parameters.\n\n\nProximal Gradient Descent:\nThis optimization algorithm is often used in sparse models and regularization. It combines gradient descent with proximal operators to enforce certain constraints.\nUse Case: Proximal Gradient Descent is useful when dealing with sparse models and regularization techniques like L1 and L2 regularization. It’s commonly used in problems where feature selection or sparsity is essential.\n\n\nLBFGS-Optimized Adam (L-BFGS-Adam):\nThis combines L-BFGS and Adam to leverage the benefits of both methods. It can be especially useful for deep learning models with large datasets.\nInput: Gradient of the loss function with respect to model parameters, regularization parameter, learning rate.\nOutput: Updated model parameters.\n\n\nNadam:\nNadam is an extension of Adam that incorporates Nesterov momentum. It aims to combine the benefits of both Nesterov and Adam optimization techniques.\n\n\nFTRL (Follow-The-Regularized-Leader):\nFTRL is an online learning algorithm often used in large-scale machine learning problems. It handles sparsity and L1 regularization efficiently.\nThese are just some of the commonly used optimizers in machine learning and deep learning. The choice of optimizer can significantly impact the training process and the final performance of a model. The selection often depends on the specific problem, architecture, and dataset being used.\nIn summary, while learning rate is a vital aspect of optimization, optimizers play a broader role in controlling the training process and updating model parameters. They are responsible for guiding the model towards finding optimal parameter values that minimize the loss function and achieve better generalization on unseen data.\n\n\nReferences:\nDeep learning via Hessian-free optimization"
  }
]