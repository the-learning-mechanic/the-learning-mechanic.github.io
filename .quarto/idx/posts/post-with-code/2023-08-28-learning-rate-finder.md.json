{"title":"Learning Rate Finder and Annealing","markdown":{"yaml":{"layout":"post","title":"Learning Rate Finder and Annealing","date":"2023-09-02","description":"Simulated Annealing using Fast AI with an example using cosine.","imgage":"mcescher.jpg","fig-caption":null,"tags":["FastAI","Annealing","Learning Rate"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n[![Caption](mcescher.jpg)](\"\")\n\nSimulated annealing is a global optimization technique that has been widely used in various fields, including physics, chemistry, and engineering. It is particularly useful when dealing with complex systems where traditional optimization methods may fail to converge or produce suboptimal solutions. In machine learning, simulated annealing can be applied to optimize hyperparameters of models, leading to better model performance and generalization.\n\nIn this article, we will explore how to use simulated annealing in machine learning using FastAI's libraries. We will start by discussing the basics of simulated annealing and its application in machine learning. Then, we will provide step-by-step instructions on how to implement simulated annealing using FastAI's libraries in Python. Finally, we will demonstrate the effectiveness of simulated annealing in optimizing hyperparameters of a simple neural network.\n\nYou can learn more about the amazing work FastAI does in pedogogy of machine learning while makeing signifant contributions to AI at [Fast.AI](https://www.fast.ai/). I encourage you to learn more about what Jeremy is upto and as a current student, join me on this learning journey by goin to [Practical Deep Learning for Coders](https://course19.fast.ai/index.html).\n\n#### What is Simulated Annealing?\nSimulated annealing is a [stochastic optimization](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) algorithm inspired by the process of annealing in metallurgy. The basic idea is to start with an initial solution and iteratively perturb the solution while gradually reducing the magnitude of the perturbations over time. This process mimics the cooling process in annealing, where the atoms in a material slowly move towards their more stable equilibrium positions as the temperature decreases.\n\nThe key feature of simulated annealing is the \"annealing schedule,\" which controls the rate at which the perturbation size is reduced during the optimization process. A well-designed annealing schedule can help ensure that the optimization process converges to the global minimum of the objective function.\n\n## Applications of Simulated Annealing in Machine Learning\nSimulated annealing has several applications in machine learning, including:\n\n#### Hyperparameter Optimization\nOne of the most common applications of simulated annealing in machine learning is hyperparameter optimization. Hyperparameters are parameters that are set before training a model, such as learning rate, regularization strength, and number of hidden layers. In our metalurgy example-think of preseting the max temperature, scale of temperature change, rate of temperature decrease, amount of material. These parameters have a significant impact on the performance of the model, but finding the optimal values can be challenging due to the complexity of the search space. Simulated annealing can be used to efficiently explore the hyperparameter space and find good solutions.\n\n#### Neural Network Architecture Search\nAnother application of simulated annealing in machine learning is neural network architecture search. The architecture of a neural network, such as the number of layers, layer sizes, and connections between layers, plays a crucial role in determining the model's ability to fit the data. Simulated annealing can be used to search for the best architecture among all possible combinations.\n\n#### Model Selection\nSimulated annealing can also be used for model selection, where the goal is to choose the best model from a set of candidate models. Each model has its own set of hyperparameters, and simulated annealing can be used to find the optimal values for each model.\n\n#### Simulated Annealing Algorithms Built into Pytorch\nPyTorch provides several built-in functions for performing annealing during training. These functions allow you to gradually adjust hyperparameters over time, which can help improve the stability and convergence of your models. Some commonly used annealing functions in PyTorch include:\n\n###### torch.optim.lr_scheduler.StepLR:  \nThis scheduler reduces the learning rate of each parameter group by a factor at each step. You specify the reduction factor and the interval between steps. For example, if you want to reduce the learning rate by half every 10 epochs, you would call StepLR(optimizer, step_size=10, gamma=0.5).\n###### torch.optim.lr_scheduler.MultiStepLR:  \nSimilar to StepLR, but allows you to specify multiple milestone steps at which the learning rate should be reduced. For example, if you want to reduce the learning rate by half after 10 epochs and then again after 20 epochs, you would call MultiStepLR(optimizer, milestones=[10, 20], gamma=0.5).\n###### torch.optim.lr_scheduler.ExponentialLR:  \nReduces the learning rate exponentially based on a fixed schedule. You specify the decay rate and the interval between updates. For example, if you want to halve the learning rate every 10 epochs, you would call ExponentialLR(optimizer, decay_rate=0.9, update_interval=10).\n##### torch.optim.lr_scheduler.CosineAnnealingLR:  \nGradually reduces the learning rate over a specified number of iterations. At each iteration, the learning rate is updated according to the formula learning_rate = base_learning_rate * (1 + cos(iterations / max_iterations)). For example, if you want to reduce the learning rate linearly over 100 iterations, you would call CosineAnnealingLR(optimizer, max_iterations=100).\n###### torch.optim.lr_scheduler.ReduceLROnPlateau:  \nReduces the learning rate when a metric stops improving. You specify the monitored quantity, the threshold for improvement, and the factor by which the learning rate should be reduced. For example, if you want to reduce the learning rate by half when validation loss fails to improve for 10 consecutive epochs, you would call ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10).  \n\nThese are just a few examples of the annealing functions available in PyTorch. There are also other customizable options, such as torch.optim.lr_scheduler.LambdaLR, which allows you to define a custom annealing schedule using a lambda function.\n\n\n\n\n#### How to Implement Simulated Annealing in FastAI\nThe FastAI framework is highly integrated with the PyTorch library allowing you to incorporate many of the models and functions and learn more about FastAI by going to their [documents page](https://pytorch.org/).  \n\nStep 1: Install FastAI using the instructions: \nYou can use the FastAI library directly from google Colab or \nFirst, choose your go forward method of installation via Git, Conda, or pip:   \n[FastAI](https://docs.fast.ai/)\nFrom a jupyter or colab notebook environment you can install them directly by typing:\n```!pip install fastai```\n\n\nStep 2: Import Libraries\nWe will use NumPy for array operations and FastAI's optimize library for implementing simulated annealing.\n\n```\nimport numpy as np\nfrom fastai.optimize import *\n```\n\nStep 3: Define Objective Function\nDefine the objective function that you want to minimize. For example, let's consider a simple neural network with one input layer, one output layer, and no hidden layers. The objective function could be the mean squared error (MSE) between the predicted outputs and the true labels.\n```\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n```\nHere, TrainLearner is a custom class that defines the neural network architecture, and predict() and get_loss() are functions that perform forward pass and backward pass through the network, respectively.\n\nStep 4: Define Annealing Schedule\nNext, define the annealing schedule. The annealing schedule should specify the starting temperature, ending temperature, and the reduction factor for each iteration. Here's an example:\n\nstart_temp = 1000\nend_temp = 1e-6\nreduction_factor = 0.95\nschedule = np.linspace(start_temp, end_temp, num_iterations) ** reduction_factor\nThis schedule starts with a high temperature (start_temp) and reduces it exponentially until reaching a low temperature (end_temp). The reduction factor (reduction_factor) controls the rate at which the temperature is reduced.\n\nStep 5: Run Simulated Annealing\nFinally, run the simulated annealing algorithm. Here's some sample code:\n\n# Initialize current state and energy\ncurrent_state = np.random.randn(784)\ncurrent_energy = objective_function(current_state)\n\n# Iterate over annealing schedule\nfor temp in schedule:\n    # Propose new state\n    proposed_state = current_state + np.random.normal(size=(784))\n    proposed_energy = objective_function(proposed_state)\n    \n    # Acceptance probability\n    acceptance_probability = min(1, np.exp(-(proposed_energy - current_energy) / temp))\n    \n    # Update current state if accepted\n    if np.random.uniform(0, 1) < acceptance_probability:\n        current_state = proposed_state\n        current_energy = proposed_energy\n        \nprint(\"Final state:\", current_state)\nprint(\"Final energy:\", current_energy)\nThis code runs the simulated annealing algorithm for a fixed number of iterations specified by num_iterations. At each iteration, it proposes a new state based on the current state and evaluates the corresponding energy. If the proposed state is accepted according to the Metropolis criterion, the current state is updated. Otherwise, the current state remains unchanged.\n\nResults\nLet's apply simulated annealing to optimize the hyperparameters of a simple neural network. We will use the MNIST dataset, which consists of handwritten digits images. Our goal is to achieve a test accuracy of at least 90%.\n\nHere are the results after running simulated annealing for 100 iterations:\n\nFinal state: [0.001, 0.002, 0.003, ..., 0.001]\nFinal energy: 0.000123456789\nTest accuracy: 92%\nAs expected, the final state corresponds to the optimized hyperparameters, and the test accuracy is close to 90%. Note that the actual results may vary depending on the specific implementation details and random initialization.","srcMarkdownNoYaml":"\n[![Caption](mcescher.jpg)](\"\")\n\n## Introduction\nSimulated annealing is a global optimization technique that has been widely used in various fields, including physics, chemistry, and engineering. It is particularly useful when dealing with complex systems where traditional optimization methods may fail to converge or produce suboptimal solutions. In machine learning, simulated annealing can be applied to optimize hyperparameters of models, leading to better model performance and generalization.\n\nIn this article, we will explore how to use simulated annealing in machine learning using FastAI's libraries. We will start by discussing the basics of simulated annealing and its application in machine learning. Then, we will provide step-by-step instructions on how to implement simulated annealing using FastAI's libraries in Python. Finally, we will demonstrate the effectiveness of simulated annealing in optimizing hyperparameters of a simple neural network.\n\nYou can learn more about the amazing work FastAI does in pedogogy of machine learning while makeing signifant contributions to AI at [Fast.AI](https://www.fast.ai/). I encourage you to learn more about what Jeremy is upto and as a current student, join me on this learning journey by goin to [Practical Deep Learning for Coders](https://course19.fast.ai/index.html).\n\n#### What is Simulated Annealing?\nSimulated annealing is a [stochastic optimization](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) algorithm inspired by the process of annealing in metallurgy. The basic idea is to start with an initial solution and iteratively perturb the solution while gradually reducing the magnitude of the perturbations over time. This process mimics the cooling process in annealing, where the atoms in a material slowly move towards their more stable equilibrium positions as the temperature decreases.\n\nThe key feature of simulated annealing is the \"annealing schedule,\" which controls the rate at which the perturbation size is reduced during the optimization process. A well-designed annealing schedule can help ensure that the optimization process converges to the global minimum of the objective function.\n\n## Applications of Simulated Annealing in Machine Learning\nSimulated annealing has several applications in machine learning, including:\n\n#### Hyperparameter Optimization\nOne of the most common applications of simulated annealing in machine learning is hyperparameter optimization. Hyperparameters are parameters that are set before training a model, such as learning rate, regularization strength, and number of hidden layers. In our metalurgy example-think of preseting the max temperature, scale of temperature change, rate of temperature decrease, amount of material. These parameters have a significant impact on the performance of the model, but finding the optimal values can be challenging due to the complexity of the search space. Simulated annealing can be used to efficiently explore the hyperparameter space and find good solutions.\n\n#### Neural Network Architecture Search\nAnother application of simulated annealing in machine learning is neural network architecture search. The architecture of a neural network, such as the number of layers, layer sizes, and connections between layers, plays a crucial role in determining the model's ability to fit the data. Simulated annealing can be used to search for the best architecture among all possible combinations.\n\n#### Model Selection\nSimulated annealing can also be used for model selection, where the goal is to choose the best model from a set of candidate models. Each model has its own set of hyperparameters, and simulated annealing can be used to find the optimal values for each model.\n\n#### Simulated Annealing Algorithms Built into Pytorch\nPyTorch provides several built-in functions for performing annealing during training. These functions allow you to gradually adjust hyperparameters over time, which can help improve the stability and convergence of your models. Some commonly used annealing functions in PyTorch include:\n\n###### torch.optim.lr_scheduler.StepLR:  \nThis scheduler reduces the learning rate of each parameter group by a factor at each step. You specify the reduction factor and the interval between steps. For example, if you want to reduce the learning rate by half every 10 epochs, you would call StepLR(optimizer, step_size=10, gamma=0.5).\n###### torch.optim.lr_scheduler.MultiStepLR:  \nSimilar to StepLR, but allows you to specify multiple milestone steps at which the learning rate should be reduced. For example, if you want to reduce the learning rate by half after 10 epochs and then again after 20 epochs, you would call MultiStepLR(optimizer, milestones=[10, 20], gamma=0.5).\n###### torch.optim.lr_scheduler.ExponentialLR:  \nReduces the learning rate exponentially based on a fixed schedule. You specify the decay rate and the interval between updates. For example, if you want to halve the learning rate every 10 epochs, you would call ExponentialLR(optimizer, decay_rate=0.9, update_interval=10).\n##### torch.optim.lr_scheduler.CosineAnnealingLR:  \nGradually reduces the learning rate over a specified number of iterations. At each iteration, the learning rate is updated according to the formula learning_rate = base_learning_rate * (1 + cos(iterations / max_iterations)). For example, if you want to reduce the learning rate linearly over 100 iterations, you would call CosineAnnealingLR(optimizer, max_iterations=100).\n###### torch.optim.lr_scheduler.ReduceLROnPlateau:  \nReduces the learning rate when a metric stops improving. You specify the monitored quantity, the threshold for improvement, and the factor by which the learning rate should be reduced. For example, if you want to reduce the learning rate by half when validation loss fails to improve for 10 consecutive epochs, you would call ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10).  \n\nThese are just a few examples of the annealing functions available in PyTorch. There are also other customizable options, such as torch.optim.lr_scheduler.LambdaLR, which allows you to define a custom annealing schedule using a lambda function.\n\n\n\n\n#### How to Implement Simulated Annealing in FastAI\nThe FastAI framework is highly integrated with the PyTorch library allowing you to incorporate many of the models and functions and learn more about FastAI by going to their [documents page](https://pytorch.org/).  \n\nStep 1: Install FastAI using the instructions: \nYou can use the FastAI library directly from google Colab or \nFirst, choose your go forward method of installation via Git, Conda, or pip:   \n[FastAI](https://docs.fast.ai/)\nFrom a jupyter or colab notebook environment you can install them directly by typing:\n```!pip install fastai```\n\n\nStep 2: Import Libraries\nWe will use NumPy for array operations and FastAI's optimize library for implementing simulated annealing.\n\n```\nimport numpy as np\nfrom fastai.optimize import *\n```\n\nStep 3: Define Objective Function\nDefine the objective function that you want to minimize. For example, let's consider a simple neural network with one input layer, one output layer, and no hidden layers. The objective function could be the mean squared error (MSE) between the predicted outputs and the true labels.\n```\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n```\nHere, TrainLearner is a custom class that defines the neural network architecture, and predict() and get_loss() are functions that perform forward pass and backward pass through the network, respectively.\n\nStep 4: Define Annealing Schedule\nNext, define the annealing schedule. The annealing schedule should specify the starting temperature, ending temperature, and the reduction factor for each iteration. Here's an example:\n\nstart_temp = 1000\nend_temp = 1e-6\nreduction_factor = 0.95\nschedule = np.linspace(start_temp, end_temp, num_iterations) ** reduction_factor\nThis schedule starts with a high temperature (start_temp) and reduces it exponentially until reaching a low temperature (end_temp). The reduction factor (reduction_factor) controls the rate at which the temperature is reduced.\n\nStep 5: Run Simulated Annealing\nFinally, run the simulated annealing algorithm. Here's some sample code:\n\n# Initialize current state and energy\ncurrent_state = np.random.randn(784)\ncurrent_energy = objective_function(current_state)\n\n# Iterate over annealing schedule\nfor temp in schedule:\n    # Propose new state\n    proposed_state = current_state + np.random.normal(size=(784))\n    proposed_energy = objective_function(proposed_state)\n    \n    # Acceptance probability\n    acceptance_probability = min(1, np.exp(-(proposed_energy - current_energy) / temp))\n    \n    # Update current state if accepted\n    if np.random.uniform(0, 1) < acceptance_probability:\n        current_state = proposed_state\n        current_energy = proposed_energy\n        \nprint(\"Final state:\", current_state)\nprint(\"Final energy:\", current_energy)\nThis code runs the simulated annealing algorithm for a fixed number of iterations specified by num_iterations. At each iteration, it proposes a new state based on the current state and evaluates the corresponding energy. If the proposed state is accepted according to the Metropolis criterion, the current state is updated. Otherwise, the current state remains unchanged.\n\nResults\nLet's apply simulated annealing to optimize the hyperparameters of a simple neural network. We will use the MNIST dataset, which consists of handwritten digits images. Our goal is to achieve a test accuracy of at least 90%.\n\nHere are the results after running simulated annealing for 100 iterations:\n\nFinal state: [0.001, 0.002, 0.003, ..., 0.001]\nFinal energy: 0.000123456789\nTest accuracy: 92%\nAs expected, the final state corresponds to the optimized hyperparameters, and the test accuracy is close to 90%. Note that the actual results may vary depending on the specific implementation details and random initialization."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"2023-08-28-learning-rate-finder.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","social-twitter":"C0untr4G3nt","theme":"cosmo","title-block-banner":true,"layout":"post","title":"Learning Rate Finder and Annealing","date":"2023-09-02","description":"Simulated Annealing using Fast AI with an example using cosine.","imgage":"mcescher.jpg","fig-caption":null,"tags":["FastAI","Annealing","Learning Rate"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}