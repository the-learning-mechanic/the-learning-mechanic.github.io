[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thank you for stopping by. This blog is my way of documenting my journey through the second semester of part 2 of the FastAI: Machine Learning for Coders\nI’ve had a successful career as a product leader and owner at amazing companies like Microsoft, Blackberry, and Lucent. I hold a bachelor’s in Physics and a Master’s in Computer Science. Some of my more notable accomplishments were the first to launch a Garmin GPS-integrated smartphone; the 3600 Curve from Blackberry and the first to launch the 920 Lumia Windows Phone with Microsoft. I’ve had the privilege of supporting product, technical sales and business operations teams leading software and hardware launches. My interest in machine learning started from working with numpy, pandas, and SQLAlchemy as a Developer Evangelist Lead at a Geo-Spatial and Analytics company. My curiosity led me to the courses at FastAI where after a couple of false starts I joined a study group as a way to learn and interact with others who share a similar interest in machine learning. The combination of my fondness for linear algebra and diff equations, accessibility of Python as a language, and last three years working as a Developer Evangelist at a GeoSpatial and Analytics Company is what motivated me to go through the FastAI course."
  },
  {
    "objectID": "posts/Annealer/FastAIlearner.html",
    "href": "posts/Annealer/FastAIlearner.html",
    "title": "Learner",
    "section": "",
    "text": "::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=6}\n:::\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom contextlib import contextmanager\nfrom torch import nn,tensor\nfrom datasets import load_dataset,load_dataset_builder\nfrom miniai.datasets import *\nfrom miniai.conv import *\nimport logging\nfrom fastcore.test import test_close\nimport numpy as np\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\nlogging.disable(logging.WARNING)\nx,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\nbs = 512\nlr = 0.2\nm,nh = 28*28,50\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\ndef get_model(): return nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([512, 784]), tensor([2, 8, 7, 3, 0, 3, 2, 8, 6, 9]))"
  },
  {
    "objectID": "posts/Annealer/FastAIlearner.html#basic-callbacks-learner",
    "href": "posts/Annealer/FastAIlearner.html#basic-callbacks-learner",
    "title": "Learner",
    "section": "Basic Callbacks Learner",
    "text": "Basic Callbacks Learner\n\nclass CancelFitException(Exception): pass\nclass CancelBatchException(Exception): pass\nclass CancelEpochException(Exception): pass\n\n\nclass Callback(): order = 0\n\n\ndef run_cbs(cbs, method_nm, learn=None):\n    for cb in sorted(cbs, key=attrgetter('order')):\n        method = getattr(cb, method_nm, None)\n        if method is not None: method(learn)"
  },
  {
    "objectID": "posts/Annealer/FastAIlearner.html#metrics",
    "href": "posts/Annealer/FastAIlearner.html#metrics",
    "title": "Learner",
    "section": "Metrics",
    "text": "Metrics\n\nclass Metric:\n    def __init__(self): self.reset()\n    def reset(self): self.vals,self.ns = [],[]\n    def add(self, inp, targ=None, n=1):\n        self.last = self.calc(inp, targ)\n        self.vals.append(self.last)\n        self.ns.append(n)\n    @property\n    def value(self):\n        ns = tensor(self.ns)\n        return (tensor(self.vals)*ns).sum()/ns.sum()\n    def calc(self, inps, targs): return inps\n    \n\n\nclass Accuracy(Metric):\n    def calc(self, inps, targs): return (inps==targs).float().mean()"
  },
  {
    "objectID": "posts/Annealer/FastAIlearner.html#some-callbacks",
    "href": "posts/Annealer/FastAIlearner.html#some-callbacks",
    "title": "Learner",
    "section": "Some callbacks",
    "text": "Some callbacks\npip install torcheval\n\nfrom torcheval.metrics import MulticlassAccuracy,Mean\n\n\ndef to_cpu(x):\n    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n    if isinstance(x, list): return [to_cpu(o) for o in x]\n    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n    res = x.detach().cpu()\n    return res.float() if res.dtype==torch.float16 else res\n\n\nclass MetricsCB(Callback):\n    def __init__(self, *ms, **metrics):\n        for o in ms: metrics[type(o).__name__] = o\n        self.metrics = metrics\n        self.all_metrics = copy(metrics)\n        self.all_metrics['loss'] = self.loss = Mean()\n\n    def _log(self, d): print(d)\n    def before_fit(self, learn): learn.metrics = self\n    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n\n    def after_epoch(self, learn):\n        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n        log['epoch'] = learn.epoch\n        log['train'] = 'train' if learn.model.training else 'eval'\n        self._log(log)\n\n    def after_batch(self, learn):\n        x,y,*_ = to_cpu(learn.batch)\n        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n        self.loss.update(to_cpu(learn.loss), weight=len(x))\n\n\nclass DeviceCB(Callback):\n    def __init__(self, device=def_device): fc.store_attr()\n    def before_fit(self, learn):\n        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)"
  },
  {
    "objectID": "posts/Annealer/FastAIlearner.html#flexible-learner",
    "href": "posts/Annealer/FastAIlearner.html#flexible-learner",
    "title": "Learner",
    "section": "Flexible learner",
    "text": "Flexible learner\n\nclass TrainCB(Callback):\n    def __init__(self, n_inp=1): self.n_inp = n_inp\n    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])\n    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])\n    def backward(self, learn): learn.loss.backward()\n    def step(self, learn): learn.opt.step()\n    def zero_grad(self, learn): learn.opt.zero_grad()\n\nNB: I added self.n_inp after the lesson. This allows us to train models with more than one input or output.\n\nclass ProgressCB(Callback):\n    order = MetricsCB.order+1\n    def __init__(self, plot=False): self.plot = plot\n    def before_fit(self, learn):\n        learn.epochs = self.mbar = master_bar(learn.epochs)\n        self.first = True\n        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n        self.losses = []\n        self.val_losses = []\n\n    def _log(self, d):\n        if self.first:\n            self.mbar.write(list(d), table=True)\n            self.first = False\n        self.mbar.write(list(d.values()), table=True)\n\n    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if self.plot and hasattr(learn, 'metrics') and learn.training:\n            self.losses.append(learn.loss.item())\n            if self.val_losses: self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n    \n    def after_epoch(self, learn): \n        if not learn.training:\n            if self.plot and hasattr(learn, 'metrics'): \n                self.val_losses.append(learn.metrics.all_metrics['loss'].compute())\n                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n\nNB: Added validation loss plotting after the lesson.\n\nbs = 1014\nlr = 0.9\nm,nh = 28*28,100\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1014, 784]), tensor([8, 8, 5, 0, 6, 2, 5, 1, 8, 4]))\n\n\n\nclass with_cbs:\n    def __init__(self, nm): self.nm = nm\n    def __call__(self, f):\n        def _f(o, *args, **kwargs):\n            try:\n                o.callback(f'before_{self.nm}')\n                f(o, *args, **kwargs)\n                o.callback(f'after_{self.nm}')\n            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n            finally: o.callback(f'cleanup_{self.nm}')\n        return _f\n\n\nclass Learner():\n    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n        cbs = fc.L(cbs)\n        fc.store_attr()\n\n    @with_cbs('batch')\n    def _one_batch(self):\n        self.predict()\n        self.callback('after_predict')\n        self.get_loss()\n        self.callback('after_loss')\n        if self.training:\n            self.backward()\n            self.callback('after_backward')\n            self.step()\n            self.callback('after_step')\n            self.zero_grad()\n\n    @with_cbs('epoch')\n    def _one_epoch(self):\n        for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n\n    def one_epoch(self, training):\n        self.model.train(training)\n        self.dl = self.dls.train if training else self.dls.valid\n        self._one_epoch()\n\n    @with_cbs('fit')\n    def _fit(self, train, valid):\n        for self.epoch in self.epochs:\n            if train: self.one_epoch(True)\n            if valid: torch.no_grad()(self.one_epoch)(False)\n\n    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs: self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            if lr is None: lr = self.lr\n            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n            self._fit(train, valid)\n        finally:\n            for cb in cbs: self.cbs.remove(cb)\n\n    def __getattr__(self, name):\n        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n        raise AttributeError(name)\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n    \n    @property\n    def training(self): return self.model.training"
  },
  {
    "objectID": "posts/Annealer/FastAIlearner.html#trainlearner-and-momentumlearner",
    "href": "posts/Annealer/FastAIlearner.html#trainlearner-and-momentumlearner",
    "title": "Learner",
    "section": "TrainLearner and MomentumLearner",
    "text": "TrainLearner and MomentumLearner\n\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n\n\nclass MomentumLearner(TrainLearner):\n    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):\n        self.mom = mom\n        super().__init__(model, dls, loss_func, lr, cbs, opt_func)\n\n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.model.parameters(): p.grad *= self.mom\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=0.1, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.693\n0.916\n0\ntrain\n\n\n0.785\n0.586\n0\neval"
  },
  {
    "objectID": "posts/Annealer/FastAIlearner.html#lrfindercb",
    "href": "posts/Annealer/FastAIlearner.html#lrfindercb",
    "title": "Learner",
    "section": "LRFinderCB",
    "text": "LRFinderCB\n\nclass CosAnnealCB(Callback):\n    def __init__(self, lr_mult=1.3): \n        fc.store_attr()\n \n    def before_fit(self, learn):\n        self.epochs, self.lrs,self.losses = [],[], []\n        self.min = math.inf\n        self.t_iter = len(learn.dls.train) * learn.n_epochs\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        c_iter = learn.iter\n        self.losses.append(loss)\n        self.epochs.append(c_iter)\n        if loss &lt; self.min: self.min = loss\n        if loss &gt; self.min*2: raise CancelFitException()\n        for g in learn.opt.param_groups: g['lr'] *= self.lr_mult\n        g['lr'] = g['lr']*max(np.cos((1-4.0*np.pi*(c_iter / self.t_iter))),1.5)\n        \n\n\nlrfind = CosAnnealCB()\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), lrfind]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-6, cbs=cbs)\nlearn.fit(1)\n\n\nplt.plot(lrfind.epochs, lrfind.lrs)\nplt.xlabel('Epochs')\nplt.ylabel('Learning Rate (Cosine)')\nplt.xscale('linear')\n\n\n\n\nFig 3 - Learning Rate Cosine Annealer\n\n\n\n\n\nplt.plot(lrfind.epochs, lrfind.losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xscale('linear')\n\n\n\n\nFig 4 - Learning Rate Cosine Annealer\n\n\n\n\n\nclass LRFinderCB(Callback):\n    def __init__(self, lr_mult=1.3): \n        fc.store_attr()\n \n    def before_fit(self, learn):\n        self.epochs, self.lrs,self.losses = [],[], []\n        self.min = math.inf\n        self.t_iter = len(learn.dls.train) * learn.n_epochs\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        c_iter = learn.iter\n        self.losses.append(loss)\n        self.epochs.append(c_iter)\n        if loss &lt; self.min: self.min = loss\n        if loss &gt; self.min*2: raise CancelFitException()\n        for g in learn.opt.param_groups: g['lr'] *= self.lr_mult\n\n\nlrfind = LRFinderCB()\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), lrfind]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-6, cbs=cbs)\nlearn.fit(1)\n\n\nplt.plot(lrfind.epochs, lrfind.lrs)\nplt.xlabel('Epochs')\nplt.ylabel('Learning Rate')\nplt.xscale('linear')\n\n\n\n\nFig 1 - Learning Rate Finder\n\n\n\n\n\nplt.plot(lrfind.epochs, lrfind.losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xscale('linear')\n\n\n\n\nFig 2 - Learning Rate Cosine Annealer"
  },
  {
    "objectID": "posts/Resources and Links/index.html",
    "href": "posts/Resources and Links/index.html",
    "title": "Resources and Links",
    "section": "",
    "text": "I’ll attempt to keep the content and links as current as possible.\n\n\n\nStuttgart City Library\n\n\n\nMachine Learning Resources:\nFastAI\nMeta AI\nHuggingface\nPyTorch\nPapers With Code\nQuarto"
  },
  {
    "objectID": "posts/Annealer/2023-08-28- FastAI Cosine Annealer.html",
    "href": "posts/Annealer/2023-08-28- FastAI Cosine Annealer.html",
    "title": "Cosine Annealing With a FastAI Learner",
    "section": "",
    "text": "Relativity by MC Escher"
  },
  {
    "objectID": "posts/Annealer/2023-08-28- FastAI Cosine Annealer.html#conclusion",
    "href": "posts/Annealer/2023-08-28- FastAI Cosine Annealer.html#conclusion",
    "title": "Cosine Annealing With a FastAI Learner",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we learned how to implement cosine annealing and accuracy calculation in a FastAI training loop. By extending the LRFinder class and creating a custom Accuracy metric, we were able to create a complete training loop that adapts the learning rate during training and prints out the accuracy at each epoch.\nWith this knowledge, you can now apply these techniques to your own deep-learning projects and improve the performance of your models."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Learning Mechanic",
    "section": "",
    "text": "Learner\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCosine Annealing With a FastAI Learner\n\n\n\n\n\nImplementing a Cosine Annealer with the FastAI learner\n\n\n\n\n\n\nSep 5, 2023\n\n\nManuel Pardo\n\n\n\n\n\n\n  \n\n\n\n\nResources and Links\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nManuel Pardo\n\n\n\n\n\n\nNo matching items"
  }
]