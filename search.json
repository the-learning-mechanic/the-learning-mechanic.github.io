[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "EXPERIENCE\nEQ Works, Toronto, ON April 2021 – Present\nProgram Manager / Technical Evangelist\nResponsible for designing and implementing product strategy and agile program execution methodologies for the Locus Geospatial Analytics Platform and Paymi Financial Rewards application.\n• Implemented A/B feature testing methodology for the Paymi application with GitHub.\n• Implemented program management and release framework for delivering ML and Visualization solutions using GitHub Projects, AWS, Jupyter Notebooks, SQL, and Asana. • Lead quarterly companywide NPI and launch activities. • Editor of the EQ Works Medium site. Delivered 4 publications one of which is my own. • Lead customer and partner acquisition and onboarding activities including building Jupyter Notebooks, pre-sales activities, and training sessions.\nWolf Advanced Technology, Toronto, ON April 2020 – Dec 2020 Business Development Engineer Owned technical relationship with Tier 1 defence contractors for Wolf’s line of GPU, SBC, and FPGA modules used in AI inference, SIGINT, and EO/IR.\n• Analyze customer’s IRAD program requirements to deliver ANSI VITA 46 and 48 compliant and SOSA Aligned modules for video, RADAR, SAR capture, processing, and AI Inferencing.\n• Lead pre-sales activities, RFPs, JAD sessions, and SOWs.\n• Sitting member and contributor to SOSA.\nMapsted, Toronto, ON May 2019 – April 2020 Sales Operations and Strategy Lead Responsible for developing the pre- and post-sales support process of Mapsted’s wayfinding analytics platform.\n• Liaised between internal and customer product teams to gather business and technical requirements to facilitate onboarding and adoption of Mapsted’s mobile geo-analytics platform. • Crafted an effective demand generation process via integration of marketing campaigns across multiple channels, including webinars, referral sites, and content syndication. Resulted in a 30 million USD Pipeline of big-box retail accounts.\n• Recruit, onboard, and mentor sales and marketing staff.\nGiesecke + Devrient - Mobile Security, Toronto, ON July 2018 – Feb 2019 Senior Account Executive\nAccountable for the technical and business relationship of top tier financial institutions in Canada.\n• Gathered and analyzed the secure payments needs of major financial institutions.\n• Developed/prepared client dashboards, scorecards and KPI’s for monthly and yearly Executive Business Reviews, RFPs and customer satisfaction surveys.\n• Managed S&OP and technology roadmap for a portfolio valued at $4 Million. Tracked towards 105% of the 2019 plan.\nPythian, Toronto, ON June 2017- December 2017 Director of Sales Enablement – Azure Cloud Led technical sales teams responsible for onboarding enterprise customers onto Azure Cloud for our managed services practice.\n• Led customer architecture and design sessions to define the optimal solution to their current migration, expansion, and cost optimization needs.\n• Delivered RFP’s and SOW’s on two major Oracle on-premise to MS SQL on Azure migrations for financial institutions.\nSOTI, Mississauga, ON 2015-2017 Director of Strategic Alliances Built a team focused on growing sales through the advancement of the Windows product platform with system integrators, OEMs and technology partners for our MDM SaaS software.\n• Built the technology strategy with IS, VAR and technology partners elevating SOTI to premier SaaS MDM partner.\n• Led UX and new feature introductions and storyboards using JIRA during sprints focused on partner and market feedback. Achieved Tier 1 partner status with Microsoft which led to the addition of exclusive features contributing to the 25% increase in revenue from Windows deployments, 100% of target and Microsoft Platinum sponsorship of the SOTI Sync customer event 2 years in a row. • Built opportunity plan in Salesforce.com by attending trade shows and industry events. Increased Windows opportunities by 25% in FY2016 on total revenue of USD 50 Million.\nMICROSOFT, Mississauga, ON 2012 – 2015 Technology Solutions Professional, Canada Led all technical and product management responsibilities for OEM, government and wireless carrier acceptance and realization testing for the Windows Smartphone and tablet across Canada.\n• Responsible for gathering, assessing and prioritizing 4G/LTE and VoIP requirements for PTCRB acceptance • Provided local development support for ISVs and technology partners which led to the provisioning of fifty plus financial, entertainment and remote monitoring apps onto the Microsoft Apps Store.\n• Led Windows Phone 8 adoption with 3 OEMs through 5 Canadian wireless telecoms resulting in an initial 3% market share increase. Accomplished by being the voice of the customer/partner.\n• Achieved 100% of target delivering 50% YoY growth between 2012 and 2013. BLACKBERRY, Waterloo, ON 2003 – 2011 Senior Manager, Business Operations, Latin America\nOwned the S&OP for the largest business desk region consisting of $250 USD in revenue per quarter, team of 3 high performing Business Managers and 40 Legal, Marketing, Finance and Sales Directors. Led process improvements in forecasting, supply chain, inventory management and contract negotiations.\n• Led deal desk to meet 25% QoQ revenue growth targets by turning 100% business case success rate into executable go-to-market strategies with wireless telecom operators.\n• Analyzed and developed sales and marketing KPI’s and PowerBI/Excel dashboards leading to increased sales and lower inventory costs. • Set up virtual teams with the regional leadership and legal. Resulting in the reduction of pending contracts by 50% to facilitate rapid execution of g-to-m activities.\nCarrier Product Manager, Latin America\n• Led all new handset and services technology roadmap for our Latin American telco partners. • Negotiated handset and marketing strategies with carrier executives, resulting in consistent slotting of Blackberries within their portfolio along with hero status in seasonal campaigns. Owned entire placement, launch strategy and execution. First to launch 8300 Curve, 8100 Pearl and Garmin GPS Application in Latin America. Launched seven other versions of BlackBerry devices.\n• Delivered innovative use of BlackBerry in agriculture, awarded Wireless Leadership Award at RIM’s annual Wireless Enterprise Symposium in 2007.\nTechnical Carrier Manager, North East U.S.\nAs SME achieved 110% of quarterly targets. Key contributor to BlackBerry’s success within the Verizon Wireless’ Enterprise Channel in the East Coast. • Collaborated with field and channel sales teams on sales planning. Led to further adoption within large accounts in financial, healthcare and industrial sectors.\n• Led pre-sales, requirements gathering, RFPs, and POC sessions with F100 Accounts. Resulted in device and server implementation by an F10 company in the energy sector and one of the top 5 pharmaceutical companies in the U.S.\n• Delivered new product presentations for VZW’s Data Sales Leadership on a regular basis.\nLUCENT TECHNOLOGIES (INS), New York, NY 2000 – 2003 Senior Network Systems Consultant Managed teams of +10 consultants during RFP/RFQ response and engagement life cycle of Infrastructure projects within Fortune 500 clients averaging 10 Million USD per project.\n• Received CEO award in 2002 for excellence in implementing entire engagement lifecycle of Active Directory and Windows 2000.\n• Led pre-sales efforts for the Microsoft Practice including, AD, Windows 2000, IIS, VPN, RADIUS.\n• Special Recognition, Q2 of 2000 and Q2 of 2002 known as At-A-Boy Awards for outstanding support of the Pfizer account as well as partner EMC. EDUCATION AND PROFESSIONAL DEVELOPMENT Masters, Computer Science, New York University Tandon School of Engineering, NY Bachelors, Physics, Stony Brook University, NY AWS Solutions Architect Associate\nAWS SysOps Administrator Associate LANGUAGES Fluent in English, Spanish, working knowledge of Brazilian Portuguese. SKILLS AWS – Networking, EMR, Athena, EKS, S3 Python – ML, Data transformation and analytics GitHub – CD/CI, Product Management Jupyter – Data Analytics Jira – CD/CI, Product Management API – Postman Docker, Kubernetes Team building Management – Technical and Analytics S&OP – ERP, Supply Chain Planning, IoT – Intune, SOTI, Blackberry, Linux Mobile – MDM, Android, Blackberry, Windows, LTE, VOLTE, 5G, VOIP/SIP,"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/2023-08-28-learning-rate-finder.html",
    "href": "posts/post-with-code/2023-08-28-learning-rate-finder.html",
    "title": "Learning Rate Finder and Annealing",
    "section": "",
    "text": "Caption"
  },
  {
    "objectID": "posts/post-with-code/2023-08-28-learning-rate-finder.html#introduction",
    "href": "posts/post-with-code/2023-08-28-learning-rate-finder.html#introduction",
    "title": "Learning Rate Finder and Annealing",
    "section": "Introduction",
    "text": "Introduction\nSimulated annealing is a global optimization technique that has been widely used in various fields, including physics, chemistry, and engineering. It is particularly useful when dealing with complex systems where traditional optimization methods may fail to converge or produce suboptimal solutions. In machine learning, simulated annealing can be applied to optimize hyperparameters of models, leading to better model performance and generalization.\nIn this article, we will explore how to use simulated annealing in machine learning using FastAI’s libraries. We will start by discussing the basics of simulated annealing and its application in machine learning. Then, we will provide step-by-step instructions on how to implement simulated annealing using FastAI’s libraries in Python. Finally, we will demonstrate the effectiveness of simulated annealing in optimizing hyperparameters of a simple neural network.\nYou can learn more about the amazing work FastAI does in pedogogy of machine learning while makeing signifant contributions to AI at Fast.AI. I encourage you to learn more about what Jeremy is upto and as a current student, join me on this learning journey by goin to Practical Deep Learning for Coders.\n\nWhat is Simulated Annealing?\nSimulated annealing is a stochastic optimization algorithm inspired by the process of annealing in metallurgy. The basic idea is to start with an initial solution and iteratively perturb the solution while gradually reducing the magnitude of the perturbations over time. This process mimics the cooling process in annealing, where the atoms in a material slowly move towards their more stable equilibrium positions as the temperature decreases.\nThe key feature of simulated annealing is the “annealing schedule,” which controls the rate at which the perturbation size is reduced during the optimization process. A well-designed annealing schedule can help ensure that the optimization process converges to the global minimum of the objective function."
  },
  {
    "objectID": "posts/post-with-code/2023-08-28-learning-rate-finder.html#applications-of-simulated-annealing-in-machine-learning",
    "href": "posts/post-with-code/2023-08-28-learning-rate-finder.html#applications-of-simulated-annealing-in-machine-learning",
    "title": "Learning Rate Finder and Annealing",
    "section": "Applications of Simulated Annealing in Machine Learning",
    "text": "Applications of Simulated Annealing in Machine Learning\nSimulated annealing has several applications in machine learning, including:\n\nHyperparameter Optimization\nOne of the most common applications of simulated annealing in machine learning is hyperparameter optimization. Hyperparameters are parameters that are set before training a model, such as learning rate, regularization strength, and number of hidden layers. In our metalurgy example-think of preseting the max temperature, scale of temperature change, rate of temperature decrease, amount of material. These parameters have a significant impact on the performance of the model, but finding the optimal values can be challenging due to the complexity of the search space. Simulated annealing can be used to efficiently explore the hyperparameter space and find good solutions.\n\n\nNeural Network Architecture Search\nAnother application of simulated annealing in machine learning is neural network architecture search. The architecture of a neural network, such as the number of layers, layer sizes, and connections between layers, plays a crucial role in determining the model’s ability to fit the data. Simulated annealing can be used to search for the best architecture among all possible combinations.\n\n\nModel Selection\nSimulated annealing can also be used for model selection, where the goal is to choose the best model from a set of candidate models. Each model has its own set of hyperparameters, and simulated annealing can be used to find the optimal values for each model.\n\n\nSimulated Annealing Algorithms Built into Pytorch\nPyTorch provides several built-in functions for performing annealing during training. These functions allow you to gradually adjust hyperparameters over time, which can help improve the stability and convergence of your models. Some commonly used annealing functions in PyTorch include:\n\ntorch.optim.lr_scheduler.StepLR:\nThis scheduler reduces the learning rate of each parameter group by a factor at each step. You specify the reduction factor and the interval between steps. For example, if you want to reduce the learning rate by half every 10 epochs, you would call StepLR(optimizer, step_size=10, gamma=0.5). ###### torch.optim.lr_scheduler.MultiStepLR:\nSimilar to StepLR, but allows you to specify multiple milestone steps at which the learning rate should be reduced. For example, if you want to reduce the learning rate by half after 10 epochs and then again after 20 epochs, you would call MultiStepLR(optimizer, milestones=[10, 20], gamma=0.5). ###### torch.optim.lr_scheduler.ExponentialLR:\nReduces the learning rate exponentially based on a fixed schedule. You specify the decay rate and the interval between updates. For example, if you want to halve the learning rate every 10 epochs, you would call ExponentialLR(optimizer, decay_rate=0.9, update_interval=10). ##### torch.optim.lr_scheduler.CosineAnnealingLR:\nGradually reduces the learning rate over a specified number of iterations. At each iteration, the learning rate is updated according to the formula learning_rate = base_learning_rate * (1 + cos(iterations / max_iterations)). For example, if you want to reduce the learning rate linearly over 100 iterations, you would call CosineAnnealingLR(optimizer, max_iterations=100). ###### torch.optim.lr_scheduler.ReduceLROnPlateau:\nReduces the learning rate when a metric stops improving. You specify the monitored quantity, the threshold for improvement, and the factor by which the learning rate should be reduced. For example, if you want to reduce the learning rate by half when validation loss fails to improve for 10 consecutive epochs, you would call ReduceLROnPlateau(optimizer, mode=‘min’, factor=0.5, patience=10).\nThese are just a few examples of the annealing functions available in PyTorch. There are also other customizable options, such as torch.optim.lr_scheduler.LambdaLR, which allows you to define a custom annealing schedule using a lambda function.\n\n\n\nHow to Implement Simulated Annealing in FastAI\nThe FastAI framework is highly integrated with the PyTorch library allowing you to incorporate many of the models and functions and learn more about FastAI by going to their documents page.\nStep 1: Install FastAI using the instructions: You can use the FastAI library directly from google Colab or First, choose your go forward method of installation via Git, Conda, or pip:\nFastAI From a jupyter or colab notebook environment you can install them directly by typing: !pip install fastai\nStep 2: Import Libraries We will use NumPy for array operations and FastAI’s optimize library for implementing simulated annealing.\nimport numpy as np\nfrom fastai.optimize import *\nStep 3: Define Objective Function Define the objective function that you want to minimize. For example, let’s consider a simple neural network with one input layer, one output layer, and no hidden layers. The objective function could be the mean squared error (MSE) between the predicted outputs and the true labels.\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\nHere, TrainLearner is a custom class that defines the neural network architecture, and predict() and get_loss() are functions that perform forward pass and backward pass through the network, respectively.\nStep 4: Define Annealing Schedule Next, define the annealing schedule. The annealing schedule should specify the starting temperature, ending temperature, and the reduction factor for each iteration. Here’s an example:\nstart_temp = 1000 end_temp = 1e-6 reduction_factor = 0.95 schedule = np.linspace(start_temp, end_temp, num_iterations) ** reduction_factor This schedule starts with a high temperature (start_temp) and reduces it exponentially until reaching a low temperature (end_temp). The reduction factor (reduction_factor) controls the rate at which the temperature is reduced.\nStep 5: Run Simulated Annealing Finally, run the simulated annealing algorithm. Here’s some sample code:"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Learning Mechanic",
    "section": "",
    "text": "Learning Rate Finder and Annealing\n\n\n\n\n\nSimulated Annealing using Fast AI with an example using cosine.\n\n\n\n\n\n\nSep 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nManuel Pardo\n\n\n\n\n\n\nNo matching items"
  }
]